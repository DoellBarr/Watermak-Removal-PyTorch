{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (0.75.0)\n",
      "Requirement already satisfied: pyngrok in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: uvicorn in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (0.17.6)\n",
      "Requirement already satisfied: nest_asyncio in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (1.5.4)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from fastapi) (1.9.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'D:\\programming\\API\\mltrain\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: starlette==0.17.1 in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from fastapi) (0.17.1)\n",
      "Requirement already satisfied: anyio<4,>=3.0.0 in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from starlette==0.17.1->fastapi) (3.5.0)\n",
      "Requirement already satisfied: PyYAML in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from pyngrok) (6.0)\n",
      "Requirement already satisfied: asgiref>=3.4.0 in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from uvicorn) (3.5.0)\n",
      "Requirement already satisfied: h11>=0.8 in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from uvicorn) (0.13.0)\n",
      "Requirement already satisfied: click>=7.0 in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from uvicorn) (8.0.4)\n",
      "Requirement already satisfied: colorama in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi) (4.1.1)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from anyio<4,>=3.0.0->starlette==0.17.1->fastapi) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\programming\\api\\mltrain\\venv\\lib\\site-packages (from anyio<4,>=3.0.0->starlette==0.17.1->fastapi) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi pyngrok uvicorn nest_asyncio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastapi import FastAPI, UploadFile\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "from torch import nn, optim\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def pil_to_np_array(pil_image):\n",
    "    array = np.array(pil_image).transpose((2, 0, 1))\n",
    "    return array.astype(np.float32) / 255.\n",
    "\n",
    "def np_to_torch_array(np_array):\n",
    "    return torch.from_numpy(np_array)[None, :]\n",
    "\n",
    "def torch_to_np_array(torch_array):\n",
    "    return torch_array.detach().cpu().numpy()[0]\n",
    "\n",
    "def read_image(path):\n",
    "    return Image.open(path)\n",
    "\n",
    "def save_image(np_array, step_, train_steps):\n",
    "    pil_image = Image.fromarray((np_array * 255.0).transpose((1, 2, 0)).astype(\"uint8\"), \"RGB\")\n",
    "    pil_image.save(f\"progress/{str(step_).zfill(len(str(train_steps)))}.png\")\n",
    "\n",
    "def get_image_grid(images, nrow=3):\n",
    "    torch_images = [torch.from_numpy(x) for x in images]\n",
    "    grid = make_grid(torch_images, nrow)\n",
    "    return grid.numpy()\n",
    "\n",
    "def visualize_sample(*images_np, nrow=3, size_factor=10):\n",
    "    c = max(x.shape[0] for x in images_np)\n",
    "    images_np = [x if (x.shape[0] == c) else np.concatenate([x, x, x], axis=0) for x in images_np]\n",
    "    grid = get_image_grid(images_np, nrow)\n",
    "    plt.figure(figsize=(len(images_np) + size_factor, 12 + size_factor))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(grid.transpose((1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def max_dimension_resize(image_pil, mask_pil, max_dim):\n",
    "    w, h = image_pil.size\n",
    "    aspect_ratio = w / h\n",
    "    if w > max_dim:\n",
    "        h = int((h / w) * max_dim)\n",
    "        w = max_dim\n",
    "    elif h > max_dim:\n",
    "        w = int(aspect_ratio * max_dim)\n",
    "        h = max_dim\n",
    "    return image_pil.resize((w, h)), mask_pil.resize((w, h))\n",
    "\n",
    "def preprocess_images(image_path, mask_path, max_dim):\n",
    "    image_pil = read_image(image_path).convert('RGB')\n",
    "    mask_pil = read_image(mask_path).convert('RGB')\n",
    "\n",
    "    image_pil, mask_pil = max_dimension_resize(image_pil, mask_pil, max_dim)\n",
    "\n",
    "    image_np = pil_to_np_array(image_pil)\n",
    "    mask_np = pil_to_np_array(mask_pil)\n",
    "\n",
    "    print('Visualizing mask overlap...')\n",
    "\n",
    "    visualize_sample(image_np, mask_np, image_np * mask_np, nrow=3, size_factor=10)\n",
    "\n",
    "    return image_np, mask_np\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class DepthwiseSeperableConv2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, **kwargs):\n",
    "        super(DepthwiseSeperableConv2d, self).__init__()\n",
    "\n",
    "        self.depthwise = nn.Conv2d(input_channels, input_channels, groups=input_channels, **kwargs)\n",
    "        self.pointwise = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, bias=False):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ReflectionPad2d(int((kernel_size - 1) / 2)),\n",
    "            DepthwiseSeperableConv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0,\n",
    "                                     bias=bias),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Concat(nn.Module):\n",
    "    def __init__(self, dim, *args):\n",
    "        super(Concat, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        inputs = [module(input_) for module in self._modules.values()]\n",
    "        inputs_shapes2 = [x.shape[2] for x in inputs]\n",
    "        inputs_shapes3 = [x.shape[3] for x in inputs]\n",
    "\n",
    "        if np.all(np.array(inputs_shapes2) == min(inputs_shapes2)) and np.all(\n",
    "            np.array(inputs_shapes3) == min(inputs_shapes3)\n",
    "        ):\n",
    "            inputs_ = inputs\n",
    "        else:\n",
    "            target_shape2 = min(inputs_shapes2)\n",
    "            target_shape3 = min(inputs_shapes3)\n",
    "\n",
    "            inputs_ = []\n",
    "            for inp in inputs:\n",
    "                diff2 = (inp.size(2) - target_shape2) // 2\n",
    "                diff3 = (inp.size(3) - target_shape3) // 2\n",
    "                inputs_.append(inp[:, :, diff2: diff2 + target_shape2, diff3:diff3 + target_shape3])\n",
    "\n",
    "        return torch.cat(inputs_, dim=self.dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._modules)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class SkipEncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_depth, num_channels_down=None, num_channels_up=None,\n",
    "                 num_channels_skip=None):\n",
    "        super(SkipEncoderDecoder, self).__init__()\n",
    "\n",
    "        if num_channels_skip is None:\n",
    "            num_channels_skip = [128] * 5\n",
    "        if num_channels_down is None:\n",
    "            num_channels_down = [128] * 5\n",
    "        if num_channels_up is None:\n",
    "            num_channels_up = [128] * 5\n",
    "        self.model = nn.Sequential()\n",
    "        model_tmp = self.model\n",
    "\n",
    "        for i in range(len(num_channels_down)):\n",
    "\n",
    "            deeper = nn.Sequential()\n",
    "            skip = nn.Sequential()\n",
    "\n",
    "            if num_channels_skip[i] != 0:\n",
    "                model_tmp.add_module(str(len(model_tmp) + 1), Concat(1, skip, deeper))\n",
    "            else:\n",
    "                model_tmp.add_module(str(len(model_tmp) + 1), deeper)\n",
    "\n",
    "            model_tmp.add_module(str(len(model_tmp) + 1), nn.BatchNorm2d(num_channels_skip[i] + (\n",
    "                num_channels_up[i + 1] if i < (len(num_channels_down) - 1) else num_channels_down[i])))\n",
    "\n",
    "            if num_channels_skip[i] != 0:\n",
    "                skip.add_module(str(len(skip) + 1), Conv2dBlock(input_depth, num_channels_skip[i], 1, bias=False))\n",
    "\n",
    "            deeper.add_module(str(len(deeper) + 1), Conv2dBlock(input_depth, num_channels_down[i], 3, 2, bias=False))\n",
    "            deeper.add_module(str(len(deeper) + 1),\n",
    "                              Conv2dBlock(num_channels_down[i], num_channels_down[i], 3, bias=False))\n",
    "\n",
    "            deeper_main = nn.Sequential()\n",
    "\n",
    "            if i == len(num_channels_down) - 1:\n",
    "                k = num_channels_down[i]\n",
    "            else:\n",
    "                deeper.add_module(str(len(deeper) + 1), deeper_main)\n",
    "                k = num_channels_up[i + 1]\n",
    "\n",
    "            deeper.add_module(str(len(deeper) + 1), nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "\n",
    "            model_tmp.add_module(str(len(model_tmp) + 1),\n",
    "                                 Conv2dBlock(num_channels_skip[i] + k, num_channels_up[i], 3, 1, bias=False))\n",
    "            model_tmp.add_module(str(len(model_tmp) + 1),\n",
    "                                 Conv2dBlock(num_channels_up[i], num_channels_up[i], 1, bias=False))\n",
    "\n",
    "            input_depth = num_channels_down[i]\n",
    "            model_tmp = deeper_main\n",
    "\n",
    "        self.model.add_module(str(len(self.model) + 1), nn.Conv2d(num_channels_up[0], 3, 1, bias=True))\n",
    "        self.model.add_module(str(len(self.model) + 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def input_noise(input_depth, spatial_size, scale=1. / 10):\n",
    "    shape = [1, input_depth, spatial_size[0], spatial_size[1]]\n",
    "    return torch.rand(*shape) * scale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def remove_watermark(image_path, mask_path, max_dim, reg_noise, input_depth, lr, show_step, training_steps,\n",
    "                     tqdm_length=100):\n",
    "    DTYPE = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "    if not torch.cuda.is_available():\n",
    "        print('\\nSetting device to \"cpu\", since torch is not built with \"cuda\" support...')\n",
    "        print('It is recommended to use GPU if possible...')\n",
    "\n",
    "    image_np, mask_np = preprocess_images(image_path, mask_path, max_dim)\n",
    "\n",
    "    print('Building the model...')\n",
    "    generator = SkipEncoderDecoder(\n",
    "        input_depth,\n",
    "        num_channels_down=[128] * 5,\n",
    "        num_channels_up=[128] * 5,\n",
    "        num_channels_skip=[128] * 5\n",
    "    ).type(DTYPE)\n",
    "\n",
    "    objective = torch.nn.MSELoss().type(DTYPE)\n",
    "    optimizer = optim.Adam(generator.parameters(), lr)\n",
    "\n",
    "    image_var = np_to_torch_array(image_np).type(DTYPE)\n",
    "    mask_var = np_to_torch_array(mask_np).type(DTYPE)\n",
    "\n",
    "    generator_input = input_noise(input_depth, image_np.shape[1:]).type(DTYPE)\n",
    "\n",
    "    generator_input_saved = generator_input.detach().clone()\n",
    "    noise = generator_input.detach().clone()\n",
    "\n",
    "    print('\\nStarting training...\\n')\n",
    "\n",
    "    progress_bar = tqdm(range(training_steps), desc='Completed', ncols=tqdm_length)\n",
    "\n",
    "    for step in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        generator_input = generator_input_saved\n",
    "\n",
    "        if reg_noise > 0:\n",
    "            generator_input = generator_input_saved + (noise.normal_() * reg_noise)\n",
    "\n",
    "        output = generator(generator_input)\n",
    "\n",
    "        loss = objective(output * mask_var, image_var * mask_var)\n",
    "        loss.backward()\n",
    "\n",
    "        if step % show_step == 0:\n",
    "            output_image = torch_to_np_array(output)\n",
    "            visualize_sample(image_np, output_image, nrow=2, size_factor=10)\n",
    "\n",
    "        progress_bar.set_postfix(Loss=loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    output_image = torch_to_np_array(output)\n",
    "    visualize_sample(output_image, nrow=1, size_factor=10)\n",
    "\n",
    "    pil_image = Image.fromarray((output_image.transpose(1, 2, 0) * 255.0).astype('uint8'))\n",
    "\n",
    "    output_path = image_path.split('/')[-1].split('.')[-2] + '-output.jpg'\n",
    "    print(f'\\nSaving final output image to: \"{output_path}\"\\n')\n",
    "\n",
    "    pil_image.save(output_path)\n",
    "    return output_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FastAPI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m app \u001B[38;5;241m=\u001B[39m \u001B[43mFastAPI\u001B[49m()\n\u001B[0;32m      2\u001B[0m \u001B[38;5;129m@app\u001B[39m\u001B[38;5;241m.\u001B[39mpost(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmain\u001B[39m(image_path: UploadFile, mask_path: UploadFile, max_dim: \u001B[38;5;28mint\u001B[39m, reg_noise: \u001B[38;5;28mfloat\u001B[39m, input_depth: \u001B[38;5;28mint\u001B[39m, lr: \u001B[38;5;28mfloat\u001B[39m,\n\u001B[0;32m      4\u001B[0m                show_step: \u001B[38;5;28mint\u001B[39m, training_steps: \u001B[38;5;28mint\u001B[39m, tqdm_length: \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m      5\u001B[0m     watermarked_extension \u001B[38;5;241m=\u001B[39m image_path\u001B[38;5;241m.\u001B[39mfilename\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'FastAPI' is not defined"
     ]
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "@app.post(\"/\")\n",
    "async def main(image_path: UploadFile, mask_path: UploadFile, max_dim: int, reg_noise: float, input_depth: int, lr: float,\n",
    "               show_step: int, training_steps: int, tqdm_length: int):\n",
    "    watermarked_extension = image_path.filename.split('.')[-1]\n",
    "    mask_extension = mask_path.filename.split('.')[-1]\n",
    "    with open(f\"watermarked.{watermarked_extension}\", \"wb\") as f:\n",
    "        f.write(await image_path.read())\n",
    "        f.close()\n",
    "    with open(f\"mask.{mask_extension}\", \"wb\") as f:\n",
    "        f.write(await mask_path.read())\n",
    "        f.close()\n",
    "    watermarked_image = remove_watermark(f\"watermarked.{watermarked_extension}\", f\"mask.{mask_extension}\", max_dim, reg_noise, input_depth, lr, show_step, training_steps,\n",
    "                     tqdm_length)\n",
    "    return {\"message\": \"success\", \"image_path\": watermarked_image}\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def main():\n",
    "    return {\"message\": \"success\"}\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}